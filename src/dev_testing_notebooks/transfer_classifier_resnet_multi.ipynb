{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /usr/remote/anaconda-3.7-2020-05-28/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /usr/remote/anaconda-3.7-2020-05-28/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /usr/remote/anaconda-3.7-2020-05-28/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /usr/remote/anaconda-3.7-2020-05-28/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /usr/remote/anaconda-3.7-2020-05-28/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /usr/remote/anaconda-3.7-2020-05-28/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /usr/remote/anaconda-3.7-2020-05-28/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /usr/remote/anaconda-3.7-2020-05-28/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import h5py\n",
    "import deepdish as dd\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt;\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import tempfile\n",
    "import time\n",
    "import pickle as pkl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "The following cell contains training parameters for the classifier. The current values were used to get our established results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdfe02d5150>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "patience_limit = 20\n",
    "patience = 0\n",
    "batch_size_train = 16# was 64\n",
    "batch_size_test = 16\n",
    "learning_rate = 0.0005\n",
    "#momentum = 0.1\n",
    "log_interval = 31\n",
    "tt_factor = 0.8\n",
    "orientation = 'AP'\n",
    "num_imgs = 5000\n",
    "\n",
    "disease_to_train = 'all_diseases_all_imgs'\n",
    "model_path = './data/saved_models/' + disease_to_train + '.pth'\n",
    "\n",
    "random_seed = 1\n",
    "#torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    NOTES: \n",
    "    - it's \"No Finding\" not \"No findings\"\n",
    "    - it's \"Pleural_Thickening\" not \"Pleural_thickening\"\n",
    "    - it's not \"Nodule Mass\", but rather \"Nodule\" and \"Mass\" separately\n",
    "'''\n",
    "disease_map = {\"Atelectasis\" : 0, \"Consolidation\" : 1, \"Infiltration\" : 2, \"Pneumothorax\": 3, \"Edema\": 4,\n",
    "               \"Emphysema\": 5, \"Fibrosis\": 6, \"Effusion\" : 7, \"Pneumonia\" : 8, \"Pleural_Thickening\" : 9,\n",
    "               \"Cardiomegaly\" : 10, \"Nodule\" : 11, \"Mass\" : 12, \"Hernia\" : 13, \"No Finding\" : 14 }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_small_map = {'Cardiomegaly': 0, 'Effusion': 1, 'Mass': 2, 'Nodule': 3, 'Atelectasis': 4, 'No Finding': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetLoader_w_val(torch.utils.data.Dataset):\n",
    "    '''\n",
    "        NOTE: I hardcoded this one a bit, basically splits validation set in half and gives it to test set\n",
    "    \n",
    "        Params: data - the data dictionary\n",
    "                view - the orientation you want to look at\n",
    "                diseases - the diseases you would like to look at\n",
    "                num_imgs - the number of images of each disease you would like\n",
    "                factor - the ratio of training and testing data\n",
    "                typ - 0 for training, 1 for testing\n",
    "    '''\n",
    "    def __init__(self, data, view, diseases, num_imgs, factor, typ, transforms=None):\n",
    "        \n",
    "        #private data\n",
    "        self.root = os.path.join('data/sorted_images',)\n",
    "        self.data = data # dict object\n",
    "        self.transforms = transforms\n",
    "        self.len_data = 0\n",
    "        datalist = []\n",
    "        \n",
    "        #Creating the datalist\n",
    "        for i in range(len(diseases)):                \n",
    "            \n",
    "            if len(data[view][diseases[i]]) <= num_imgs: #if the folder has less images than the desired number of images\n",
    "                if typ == 0:\n",
    "                    start = 0\n",
    "                    end = int(len(data[view][diseases[i]])*factor)\n",
    "                elif typ == 1:\n",
    "                    start = int(len(data[view][diseases[i]])*factor)\n",
    "                    end = int(len(data[view][diseases[i]])*factor + len(data[view][diseases[i]])*(1-factor) / 2)\n",
    "                else:\n",
    "                    start = int(len(data[view][diseases[i]])*factor + len(data[view][diseases[i]])*(1-factor) / 2)\n",
    "                    end = -1\n",
    "            else:\n",
    "                if typ == 0:\n",
    "                    start = 0\n",
    "                    end = int(num_imgs*factor)\n",
    "                elif typ == 1:\n",
    "                    start = int(num_imgs*factor)\n",
    "                    end = int(num_imgs*factor + num_imgs*(1-factor) / 2)\n",
    "                else:\n",
    "                    start = int(num_imgs*factor + num_imgs*(1-factor) / 2)\n",
    "                    end = num_imgs\n",
    "                    \n",
    "            #print('dis: ', diseases[i], ', type: ', typ, ', start: ', start, ', end:', end)\n",
    "            print('disease: ', diseases[i], 'num images used: ', min(len(data[view][diseases[i]]), num_imgs))\n",
    "            \n",
    "            #class_weights[disease_map[diseases[i]]] = num_imgs / min(len(data[view][diseases[i]]), num_imgs)\n",
    "            \n",
    "            datalist.append(self.data[view][diseases[i]][start:end])\n",
    "        \n",
    "        for item in datalist:\n",
    "            self.len_data += len(item)\n",
    "        \n",
    "        \n",
    "        self.img_paths = []\n",
    "        self.img_labels = []\n",
    "        \n",
    "        for dis in datalist:\n",
    "            for data in dis:\n",
    "                #creating the image path\n",
    "                data['img_path'] = os.path.join(self.root, data['classes'][0], view, data['img_name'])            \n",
    "                diseases_item = data['classes']\n",
    "\n",
    "                one_hot = np.zeros(6)\n",
    "                for d in diseases_item:\n",
    "                    if d in diseases:\n",
    "                        hot_index = dis_small_map[d]\n",
    "                        one_hot[hot_index] = 1\n",
    "\n",
    "                self.img_paths.append(data['img_path'])\n",
    "                self.img_labels.append(torch.Tensor(one_hot))\n",
    "\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        img_path, img_label = self.img_paths[item], self.img_labels[item]\n",
    "  \n",
    "        # TODO: fix this hot fix -> recreate data object with underscore in name\n",
    "        img_path = img_path.replace('No Finding', 'No_Finding')\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "        except:\n",
    "            img_path = img_path.replace('/AP/', '/PA/')\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "        self.cur_img_path = img_path\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            for t in self.transforms:\n",
    "                img = t(img)\n",
    "\n",
    "        return img, img_label\n",
    "    \n",
    "    def get_img_path(self):\n",
    "        return self.cur_img_path\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load in datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded in successfully\n"
     ]
    }
   ],
   "source": [
    "pkl_load = open('dataset.pickle', 'rb')\n",
    "data = pkl.load(pkl_load)\n",
    "pkl_load.close()\n",
    "print(\"data loaded in successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disease:  Cardiomegaly num images used:  2776\n",
      "disease:  Effusion num images used:  5000\n",
      "disease:  Mass num images used:  5000\n",
      "disease:  Nodule num images used:  5000\n",
      "disease:  Atelectasis num images used:  5000\n",
      "disease:  No Finding num images used:  5000\n",
      "disease:  Cardiomegaly num images used:  2776\n",
      "disease:  Effusion num images used:  5000\n",
      "disease:  Mass num images used:  5000\n",
      "disease:  Nodule num images used:  5000\n",
      "disease:  Atelectasis num images used:  5000\n",
      "disease:  No Finding num images used:  5000\n",
      "disease:  Cardiomegaly num images used:  2776\n",
      "disease:  Effusion num images used:  5000\n",
      "disease:  Mass num images used:  5000\n",
      "disease:  Nodule num images used:  5000\n",
      "disease:  Atelectasis num images used:  5000\n",
      "disease:  No Finding num images used:  5000\n",
      "22220\n",
      "2778\n",
      "2777\n"
     ]
    }
   ],
   "source": [
    "transforms = [torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "              torchvision.transforms.RandomAffine(5, translate=(0.1, 0.05), shear=0.1, scale=(0.15, 0.15)),\n",
    "              torchvision.transforms.ToTensor()]\n",
    "              #]\n",
    "#0 = train, 1 = test\n",
    "dataset_train = GetLoader_w_val(data, orientation, list(['Cardiomegaly', 'Effusion', 'Mass', 'Nodule', 'Atelectasis', 'No Finding']),\n",
    "                          num_imgs, tt_factor, 0, transforms)\n",
    "dataset_valid = GetLoader_w_val(data, orientation, list(['Cardiomegaly', 'Effusion', 'Mass', 'Nodule', 'Atelectasis', 'No Finding']),\n",
    "                          num_imgs, tt_factor, 1, transforms)\n",
    "dataset_test = GetLoader_w_val(data, orientation, list(['Cardiomegaly', 'Effusion', 'Mass', 'Nodule', 'Atelectasis', 'No Finding']),\n",
    "                          num_imgs, tt_factor, 2, transforms)\n",
    "print(len(dataset_train))\n",
    "print(len(dataset_valid))\n",
    "print(len(dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1389\n",
      "174\n",
      "174\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size_train, shuffle=True, num_workers=1)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size_train, shuffle=True, num_workers=1)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset_valid, batch_size=batch_size_train, shuffle=True, num_workers=1)\n",
    "print(len(train_loader))\n",
    "print(len(valid_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/npp002/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    }
   ],
   "source": [
    "network = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "state_dict = torch.load(model_path)\n",
    "new_state_dict = OrderedDict()\n",
    "for k,v in state_dict.items():\n",
    "    new_state_dict['module.' + k] = state_dict[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.fc = nn.Sequential(*[\n",
    "    nn.Linear(in_features=512, out_features=6, bias=True),\n",
    "    nn.Sigmoid(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import OrderedDict\n",
    "# state_dict = torch.load(model_path)\n",
    "# new_state_dict = OrderedDict()\n",
    "# for k,v in state_dict.items():\n",
    "#     new_state_dict[k[7:]] = state_dict[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import OrderedDict\n",
    "# state_dict = torch.load(model_path)\n",
    "# new_state_dict = OrderedDict()\n",
    "# for k,v in state_dict.items():\n",
    "#     new_state_dict['module.' + k] = state_dict[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys loaded in successfully\n",
      "Trainable params: 11179590\n"
     ]
    }
   ],
   "source": [
    "#network = torchvision.models.vgg11(pretrained=False)\n",
    "#network.fc = nn.Linear(512, 15)\n",
    "network = torch.nn.DataParallel(network)\n",
    "network.cuda()\n",
    "\n",
    "\n",
    "preload = True\n",
    "\n",
    "if preload:\n",
    "    network.load_state_dict(torch.load(model_path))\n",
    "    print('keys loaded in successfully')\n",
    "    \n",
    "optimizer = optim.Adam(network.parameters(), lr=learning_rate)\n",
    "print('Trainable params: {}'.format(sum(p.numel() for p in network.parameters() if p.requires_grad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(new_state_dict, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preload = True\n",
    "\n",
    "if preload:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print('keys loaded in successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## source: https://gist.github.com/the-bass/cae9f3976866776dea17a5049013258d\n",
    "def hard_binary_accuracy(batch, labels):\n",
    "    batch = torch.round(batch)\n",
    "    confusion_matrix = batch / labels\n",
    "    \n",
    "    \"\"\" Returns the confusion matrix for the values in the `prediction` and `truth`\n",
    "    tensors, i.e. the amount of positions where the values of `prediction`\n",
    "    and `truth` are\n",
    "    - 1 and 1 (True Positive)\n",
    "    - 1 and 0 (False Positive)\n",
    "    - 0 and 0 (True Negative)\n",
    "    - 0 and 1 (False Negative)\n",
    "    \"\"\"\n",
    "\n",
    "    confusion_matrix = batch / labels\n",
    "    # Element-wise division of the 2 tensors returns a new tensor which holds a\n",
    "    # unique value for each case:\n",
    "    #   1     where prediction and truth are 1 (True Positive)\n",
    "    #   inf   where prediction is 1 and truth is 0 (False Positive)\n",
    "    #   nan   where prediction and truth are 0 (True Negative)\n",
    "    #   0     where prediction is 0 and truth is 1 (False Negative)\n",
    "\n",
    "    true_positives = torch.sum(confusion_matrix == 1).item()\n",
    "    false_positives = torch.sum(confusion_matrix == float('inf')).item()\n",
    "    true_negatives = torch.sum(torch.isnan(confusion_matrix)).item()\n",
    "    false_negatives = torch.sum(confusion_matrix == 0).item()\n",
    "\n",
    "    return true_positives, false_positives, true_negatives, false_negatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 0, 4, 0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.Tensor([[[1, 1], [0, 0]], \n",
    "                  [[1, 1], [0, 0]]])\n",
    "\n",
    "l = torch.Tensor([[[1, 1], [0, 0]], \n",
    "                  [[1, 1], [0, 0]]])\n",
    "hard_binary_accuracy(b, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "\n",
    "    network.train()\n",
    "    train_losses = []\n",
    "    num_tested = 0\n",
    "    net_loss = 0\n",
    "    \n",
    "    total_true_pos, total_false_pos, total_true_neg, total_false_neg = 0, 0, 0, 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):  # (output - actual )\n",
    "        \n",
    "        if torch.cuda.is_available:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        output = network(data)\n",
    "        \n",
    "        #criterion = torch.nn.BCELoss(weight=torch.Tensor(class_weights).cuda())  # 1 0 1 0 0 0 \n",
    "        criterion = torch.nn.BCELoss()\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        net_loss += loss.clone().detach().cpu().item()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        num_tested += len(data)\n",
    "        \n",
    "        #if batch_idx % 1 == 0 or batch_idx == len(train_loader) - 1:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = output.clone().detach().clone().cpu()\n",
    "        target = target.cpu()\n",
    "\n",
    "        true_pos, false_pos, true_neg, false_neg = hard_binary_accuracy(pred, target)\n",
    "        total_true_pos += true_pos\n",
    "        total_false_pos += false_pos\n",
    "        total_true_neg += true_neg\n",
    "        total_false_neg += false_neg\n",
    "            \n",
    "    net_loss /= (batch_size_train * len(train_loader))\n",
    "    return net_loss, total_true_pos, total_false_pos, total_true_neg, total_false_neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    \n",
    "    network.eval()\n",
    "    test_losses = []\n",
    "    net_loss = 0    \n",
    "\n",
    "    total_true_pos, total_false_pos, total_true_neg, total_false_neg = 0, 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            if torch.cuda.is_available:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "            output = network(data)\n",
    "            \n",
    "            #criterion = torch.nn.BCELoss(weight=torch.Tensor(class_weights).cuda())\n",
    "            criterion = torch.nn.BCELoss()\n",
    "\n",
    "            net_loss += criterion(output, target).item()\n",
    "\n",
    "            pred = output.detach().clone().cpu()\n",
    "            target = target.cpu()\n",
    "            \n",
    "            true_pos, false_pos, true_neg, false_neg = hard_binary_accuracy(pred, target)\n",
    "            total_true_pos += true_pos\n",
    "            total_false_pos += false_pos\n",
    "            total_true_neg += true_neg\n",
    "            total_false_neg += false_neg\n",
    "            \n",
    "        \n",
    "    net_loss /= (batch_size_test * len(test_loader))\n",
    "    return net_loss, total_true_pos, total_false_pos, total_true_neg, total_false_neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "GeForce RTX 2080 Ti\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22224 2784\n"
     ]
    }
   ],
   "source": [
    "train_sample_size = len(train_loader) * batch_size_train\n",
    "test_sample_size = len(valid_loader) * batch_size_test\n",
    "print(train_sample_size, test_sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "This cell will start the training process. It will take some time, probably a few hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "-----------Epoch 1 (time = 738.69 s  aka   12.31  mins) ----------------\n",
      "Train set: Avg. loss: 0.0314,  Accuracy: 103487/133320 (77.62%)\n",
      "Test set:  Avg. loss: 0.0306, Accuracy: 12962/16662 (77.79%)\n",
      "-----------Epoch 2 (time = 717.99 s  aka   11.97  mins) ----------------\n",
      "Train set: Avg. loss: 0.0314,  Accuracy: 103510/133320 (77.64%)\n",
      "Test set:  Avg. loss: 0.0306, Accuracy: 12972/16662 (77.85%)\n",
      "-----------Epoch 3 (time = 717.27 s  aka   11.95  mins) ----------------\n",
      "Train set: Avg. loss: 0.0315,  Accuracy: 103537/133320 (77.66%)\n",
      "Test set:  Avg. loss: 0.0326, Accuracy: 12702/16662 (76.23%)\n",
      "-----------Epoch 4 (time = 717.18 s  aka   11.95  mins) ----------------\n",
      "Train set: Avg. loss: 0.0314,  Accuracy: 103467/133320 (77.61%)\n",
      "Test set:  Avg. loss: 0.0308, Accuracy: 12865/16662 (77.21%)\n",
      "-----------Epoch 5 (time = 716.04 s  aka   11.93  mins) ----------------\n",
      "Train set: Avg. loss: 0.0314,  Accuracy: 103437/133320 (77.59%)\n",
      "Test set:  Avg. loss: 0.0310, Accuracy: 12900/16662 (77.42%)\n",
      "-----------Epoch 6 (time = 714.45 s  aka   11.91  mins) ----------------\n",
      "Train set: Avg. loss: 0.0314,  Accuracy: 103453/133320 (77.60%)\n",
      "Test set:  Avg. loss: 0.0317, Accuracy: 12845/16662 (77.09%)\n",
      "-----------Epoch 7 (time = 713.61 s  aka   11.89  mins) ----------------\n",
      "Train set: Avg. loss: 0.0315,  Accuracy: 103366/133320 (77.53%)\n",
      "Test set:  Avg. loss: 0.0310, Accuracy: 12868/16662 (77.23%)\n",
      "-----------Epoch 8 (time = 711.96 s  aka   11.87  mins) ----------------\n",
      "Train set: Avg. loss: 0.0314,  Accuracy: 103447/133320 (77.59%)\n",
      "Test set:  Avg. loss: 0.0312, Accuracy: 12889/16662 (77.36%)\n",
      "-----------Epoch 9 (time = 712.15 s  aka   11.87  mins) ----------------\n",
      "Train set: Avg. loss: 0.0315,  Accuracy: 103410/133320 (77.57%)\n",
      "Test set:  Avg. loss: 0.0316, Accuracy: 12837/16662 (77.04%)\n",
      "-----------Epoch 10 (time = 713.24 s  aka   11.89  mins) ----------------\n",
      "Train set: Avg. loss: 0.0313,  Accuracy: 103508/133320 (77.64%)\n",
      "Test set:  Avg. loss: 0.0307, Accuracy: 12916/16662 (77.52%)\n",
      "-----------Epoch 11 (time = 711.92 s  aka   11.87  mins) ----------------\n",
      "Train set: Avg. loss: 0.0316,  Accuracy: 103372/133320 (77.54%)\n",
      "Test set:  Avg. loss: 0.0318, Accuracy: 12846/16662 (77.10%)\n",
      "-----------Epoch 12 (time = 711.89 s  aka   11.86  mins) ----------------\n",
      "Train set: Avg. loss: 0.0315,  Accuracy: 103459/133320 (77.60%)\n",
      "Test set:  Avg. loss: 0.0308, Accuracy: 12891/16662 (77.37%)\n",
      "-----------Epoch 13 (time = 711.58 s  aka   11.86  mins) ----------------\n",
      "Train set: Avg. loss: 0.0313,  Accuracy: 103468/133320 (77.61%)\n",
      "Test set:  Avg. loss: 0.0309, Accuracy: 12912/16662 (77.49%)\n",
      "-----------Epoch 14 (time = 712.11 s  aka   11.87  mins) ----------------\n",
      "Train set: Avg. loss: 0.0316,  Accuracy: 103325/133320 (77.50%)\n",
      "Test set:  Avg. loss: 0.0314, Accuracy: 12935/16662 (77.63%)\n",
      "-----------Epoch 15 (time = 711.79 s  aka   11.86  mins) ----------------\n",
      "Train set: Avg. loss: 0.0315,  Accuracy: 103446/133320 (77.59%)\n",
      "Test set:  Avg. loss: 0.0307, Accuracy: 12950/16662 (77.72%)\n",
      "-----------Epoch 16 (time = 711.0 s  aka   11.85  mins) ----------------\n",
      "Train set: Avg. loss: 0.0316,  Accuracy: 103396/133320 (77.55%)\n",
      "Test set:  Avg. loss: 0.0314, Accuracy: 12945/16662 (77.69%)\n",
      "-----------Epoch 17 (time = 708.75 s  aka   11.81  mins) ----------------\n",
      "Train set: Avg. loss: 0.0312,  Accuracy: 103572/133320 (77.69%)\n",
      "Test set:  Avg. loss: 0.0315, Accuracy: 12767/16662 (76.62%)\n",
      "-----------Epoch 18 (time = 710.86 s  aka   11.85  mins) ----------------\n",
      "Train set: Avg. loss: 0.0316,  Accuracy: 103393/133320 (77.55%)\n",
      "Test set:  Avg. loss: 0.0314, Accuracy: 12950/16662 (77.72%)\n",
      "-----------Epoch 19 (time = 711.28 s  aka   11.85  mins) ----------------\n",
      "Train set: Avg. loss: 0.0315,  Accuracy: 103378/133320 (77.54%)\n",
      "Test set:  Avg. loss: 0.0309, Accuracy: 12875/16662 (77.27%)\n",
      "-----------Epoch 20 (time = 711.18 s  aka   11.85  mins) ----------------\n",
      "Train set: Avg. loss: 0.0315,  Accuracy: 103434/133320 (77.58%)\n",
      "Test set:  Avg. loss: 0.0314, Accuracy: 12804/16662 (76.85%)\n",
      "Breaking on patience=20\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-350ce4957516>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reloading best model from epoch {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_epoch' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Starting Training...\")\n",
    "begin_time = time.time()\n",
    "\n",
    "min_loss_test = 0.0295\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "train_true_pos = []\n",
    "train_false_pos = []\n",
    "train_true_neg = []\n",
    "train_false_neg = []\n",
    "\n",
    "test_true_pos = []\n",
    "test_false_pos = []\n",
    "test_true_neg = []\n",
    "test_false_neg = []\n",
    "\n",
    "best_model_file = tempfile.NamedTemporaryFile(mode='w+b', delete=False)\n",
    "torch.save(network.state_dict(), best_model_file.name)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    start_time = time.time()\n",
    "    network.load_state_dict(torch.load(best_model_file.name))\n",
    "\n",
    "    loss_train, true_pos_train, false_pos_train, true_neg_train, false_neg_train = train(epoch)\n",
    "    loss_test, true_pos_test, false_pos_test, true_neg_test, false_neg_test = test()\n",
    "    \n",
    "    #scheduler.step(loss_test)\n",
    "    \n",
    "    #print(\"tp / fp / tn / fn : \", true_pos_test, false_pos_test, true_neg_test, false_neg_test )\n",
    "    train_losses.append(loss_train)\n",
    "    test_losses.append(loss_test)\n",
    "    \n",
    "    n_correct_train = true_pos_train + true_neg_train\n",
    "    n_correct_test = true_pos_test + true_neg_test\n",
    "    \n",
    "    acc_train = 100. * n_correct_train / (n_correct_train + false_pos_train + false_neg_train)\n",
    "    acc_test = 100. * n_correct_test / (n_correct_test + false_pos_test + false_neg_test)\n",
    "    \n",
    "    train_acc.append(acc_train)\n",
    "    test_acc.append(acc_test)\n",
    "    \n",
    "#     true_pos_rate_train = 100. * true_pos_train / (true_pos_train + false_pos_train)\n",
    "#     true_neg_rate_train = 100. * true_neg_train / (true_neg_train + false_neg_train)\n",
    "#     false_pos_rate_train = 100. * false_pos_train / (false_pos_train + true_neg_train)\n",
    "#     false_neg_rate_train = 100. * false_neg_train / (false_neg_train + true_pos_train)\n",
    "    \n",
    "#     true_pos_rate_test = 100. * true_pos_test / (true_pos_test + false_pos_test)\n",
    "#     true_neg_rate_test = 100. * true_neg_test / (true_neg_test + false_neg_test)\n",
    "#     false_pos_rate_test = 100. * false_pos_test / (false_pos_test + true_neg_test)\n",
    "#     false_neg_rate_test = 100. * false_neg_test / (false_neg_test + true_pos_test)\n",
    "\n",
    "    train_true_pos.append(true_pos_train)\n",
    "    train_false_pos.append(false_pos_train)\n",
    "    train_true_neg.append(true_neg_train)\n",
    "    train_false_neg.append(false_neg_train)\n",
    "\n",
    "    test_true_pos.append(true_pos_test)\n",
    "    test_false_pos.append(false_pos_test)\n",
    "    test_true_neg.append(true_neg_test)\n",
    "    test_false_neg.append(false_neg_test)\n",
    "        \n",
    "    auc_train = roc_auc_score\n",
    "\n",
    "        \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(\"-----------Epoch \" + str(epoch) + \" (time =\", round(elapsed_time, 2), \"s  aka  \", round(elapsed_time / 60, 2), \" mins) ----------------\")\n",
    "    print('Train set: Avg. loss: {:.4f},  Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "        loss_train, n_correct_train, (n_correct_train + false_pos_train + false_neg_train), acc_train))\n",
    "#     print('''\n",
    "#           True Positive: {:.2f}%\n",
    "#           True Negative: {:.2f}%\n",
    "#           False Positive: {:.2f}%\n",
    "#           False Negative {:.2f}%\n",
    "#           '''.format(true_pos_rate_train, true_neg_rate_train, false_pos_rate_train, false_neg_rate_train))    \n",
    "    print('Test set:  Avg. loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "        loss_test, n_correct_test, (n_correct_test + false_pos_test + false_neg_test), acc_test))\n",
    "#     print('''\n",
    "#           True Positive: {:.2f}%\n",
    "#           True Negative: {:.2f}%\n",
    "#           False Positive: {:.2f}%\n",
    "#           False Negative {:.2f}%\n",
    "#           '''.format(true_pos_rate_test, true_neg_rate_test, false_pos_rate_test, false_neg_rate_test))\n",
    "    \n",
    "    \n",
    "    if loss_test < min_loss_test:\n",
    "        min_loss_test = loss_test\n",
    "        patience = 0\n",
    "        best_epoch = epoch\n",
    "        \n",
    "        print('Epoch {} --- saving new best model: test loss = {}\\n'.format(epoch, loss_test))\n",
    "        torch.save(network.state_dict(), best_model_file.name)\n",
    "        torch.save(network.state_dict(), os.path.join(model_path)) # guarantees best one saved if progra crashes\n",
    "    else:\n",
    "        patience += 1\n",
    "        \n",
    "    if patience == patience_limit:\n",
    "        print('Breaking on patience={}'.format(patience))\n",
    "        break\n",
    "        \n",
    "\n",
    "print('Reloading best model from epoch {}'.format(best_epoch))\n",
    "network.load_state_dict(torch.load(best_model_file.name))\n",
    "\n",
    "end_time = time.time() - begin_time\n",
    "\n",
    "print(\"Training complete (time =\", round(end_time, 2), \"s)\")\n",
    "torch.save(network.state_dict(), \n",
    "           os.path.join(model_path))\n",
    "print(\"Saved model to {}\".format(model_path))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and Graphs\n",
    "\n",
    "These cells give an idea of the results of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Loss per Epoch\")\n",
    "plt.plot(train_losses, \"-b\",  label=\"train\")\n",
    "plt.plot(test_losses, \"-r\", label=\"test\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Accuracy per Epoch\")\n",
    "plt.plot(train_acc, \"-b\", label=\"train\")\n",
    "plt.plot(test_acc, \"-r\", label=\"test\")\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
